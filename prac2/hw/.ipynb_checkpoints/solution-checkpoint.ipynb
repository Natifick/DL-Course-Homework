{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPyPkdq5RH94"
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5-hnsEiPIz9"
   },
   "outputs": [],
   "source": [
    "class Exp(Function):\n",
    "  \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "  @staticmethod\n",
    "  def forward(ctx, i):\n",
    "    \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "    \"\"\"\n",
    "    result = i.exp()\n",
    "    ctx.save_for_backward(result)\n",
    "    return result\n",
    "\n",
    "  @staticmethod\n",
    "  def backward(ctx, grad_output):\n",
    "    \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "    \"\"\"\n",
    "    print(ctx.saved_tensors)\n",
    "    result, = ctx.saved_tensors\n",
    "    return grad_output * result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "elaOA8bdRiMc",
    "outputId": "dbdec323-534b-40e2-b2d0-ad4b6e1dd956"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.3891, grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use it by calling the apply method\n",
    "input = torch.tensor(2.0, requires_grad=True)\n",
    "output = Exp.apply(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JlcZLt-RSGgG",
    "outputId": "4af4b18f-d2ed-450a-d86d-7ac6f5f8dfa7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.38905609893065"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.exp(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Om6dn414SQeA",
    "outputId": "9b16cba0-46cb-4b74-e0b0-35117af043e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(7.3891, grad_fn=<ExpBackward>),)\n",
      "---\n",
      "data - 7.389056205749512\n",
      "grad - None\n",
      "grad_fn - <torch.autograd.function.ExpBackward object at 0x7f76064c6b80>\n",
      "req_grad - True\n",
      "is_leaf - False\n",
      "---\n",
      "data - 2.0\n",
      "grad - 7.389056205749512\n",
      "grad_fn - None\n",
      "req_grad - True\n",
      "is_leaf - True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:477.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "output.backward()\n",
    "show_tensor_params(output)\n",
    "show_tensor_params(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_14IqfeSnLM"
   },
   "source": [
    "**Задание**: реализуйте backward для Polynomial 0.5 * (5 * input ** 3 - 3 * input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5cNegVYOd8u"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Polynomial(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary\n",
    "        objects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return (7.5 * input ** 2 - 1.5) * grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fA2PNhudUNij"
   },
   "source": [
    "Практическое задание: написать собственный движок автоматического дифференцирования, а именно: реализовать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chDdD9oSUlUJ"
   },
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \"\"\" stores a single scalar value and its gradient \"\"\"\n",
    "\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._backward = lambda: None # function \n",
    "        self._prev = set(_children) # set of Value objects\n",
    "        self._op = _op # the op that produced this node, string ('+', '-', ....)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(other.data + self.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(other.data * self.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * other.data\n",
    "            other.grad += out.grad * self.data\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        out = Value(self.data ** other, (self,), '**')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * other * (self.data ** (other - 1))\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        out = Value(self.data if self.data > 0 else 0, (self,), 'relu')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad if self.data > 0 else 0\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "        # topological order all of the children in the graph\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        self.grad = 1\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other): # other - self\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other): # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other): # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "    def __rtruediv__(self, other): # other / self\n",
    "        return other * self**-1\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vY7OzWjuUiaa"
   },
   "outputs": [],
   "source": [
    "def test_sanity_check():\n",
    "\n",
    "    x = Value(-4.0)\n",
    "    z = 2 * x + 2 + x\n",
    "  \n",
    "    q = z.relu() + z * x\n",
    "    h = (z * z).relu()\n",
    "    y = h + q + q * x\n",
    "    y.backward()\n",
    "    xmg, ymg = x, y\n",
    "\n",
    "    x = torch.Tensor([-4.0]).double()\n",
    "    x.requires_grad = True\n",
    "    z = 2 * x + 2 + x\n",
    "    q = z.relu() + z * x\n",
    "    h = (z * z).relu()\n",
    "    y = h + q + q * x\n",
    "    y.backward()\n",
    "    xpt, ypt = x, y\n",
    "\n",
    "    \n",
    "    # forward pass went well\n",
    "    assert ymg.data == ypt.data.item()\n",
    "    # backward pass went well\n",
    "    print(xmg, xpt, xpt.grad)\n",
    "    assert xmg.grad == xpt.grad.item()\n",
    "\n",
    "\n",
    "def test_more_ops():\n",
    "\n",
    "    a = Value(-4.0)\n",
    "    b = Value(2.0)\n",
    "    c = a + b\n",
    "    d = a * b + b**3\n",
    "    c += c + 1\n",
    "    c += 1 + c + (-a)\n",
    "    d += d * 2 + (b + a).relu()\n",
    "    d += 3 * d + (b - a).relu()\n",
    "    e = c - d\n",
    "    f = e**2\n",
    "    g = f / 2.0\n",
    "    g += 10.0 / f\n",
    "    g.backward()\n",
    "    amg, bmg, gmg = a, b, g\n",
    "\n",
    "    a = torch.Tensor([-4.0]).double()\n",
    "    b = torch.Tensor([2.0]).double()\n",
    "    a.requires_grad = True\n",
    "    b.requires_grad = True\n",
    "    c = a + b\n",
    "    d = a * b + b**3\n",
    "    c = c + c + 1\n",
    "    c = c + 1 + c + (-a)\n",
    "    d = d + d * 2 + (b + a).relu()\n",
    "    d = d + 3 * d + (b - a).relu()\n",
    "    e = c - d\n",
    "    f = e**2\n",
    "    g = f / 2.0\n",
    "    g = g + 10.0 / f\n",
    "    g.backward()\n",
    "    apt, bpt, gpt = a, b, g\n",
    "\n",
    "    tol = 1e-6\n",
    "    # forward pass went well\n",
    "    assert abs(gmg.data - gpt.data.item()) < tol\n",
    "    # backward pass went well\n",
    "    assert abs(amg.grad - apt.grad.item()) < tol\n",
    "    assert abs(bmg.grad - bpt.grad.item()) < tol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LgTiYeZ-WGk"
   },
   "outputs": [],
   "source": [
    "a = Value(-4.0)\n",
    "b = Value(2.0)\n",
    "d = Value(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y0svSAs2h0Ap"
   },
   "outputs": [],
   "source": [
    "c = a + b\n",
    "e = c * d\n",
    "e.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9n8DN6RYkrx",
    "outputId": "443147d0-b200-436d-90d0-4f9db9a30fd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=-4.0, grad=46.0) tensor([-4.], dtype=torch.float64, requires_grad=True) tensor([46.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "test_sanity_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1T198QDQYh_q"
   },
   "outputs": [],
   "source": [
    "test_more_ops()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-KbDOhMYHZ1"
   },
   "source": [
    "# Обучение на основе собственной бибилотеки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVK1JLXom0Ze"
   },
   "source": [
    "## Многослойный перцептрон на основе класса Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkl70dxhkcQN"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Module:\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Neuron(Module):\n",
    "    #                  nin hao ma\n",
    "    def __init__(self, nin, nonlin=True):\n",
    "        self.w = [Value(1) for i in range(nin)]\n",
    "        self.b = Value(0)\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "    def __call__(self, x):\n",
    "        mid = sum([self.w[i] * x[i] for i in range(len(x))]) + self.b\n",
    "        act = mid.relu() if self.nonlin else mid\n",
    "        return act\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"\n",
    "\n",
    "class Layer(Module):\n",
    "\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        self.neurons = [Neuron(nin, kwargs['nonlin']) for i in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        out = [n(x) for n in self.neurons]\n",
    "        return out[0] if len(out) == 1 else out\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for n in self.neurons:\n",
    "            params += n.parameters()\n",
    "        return params\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n",
    "\n",
    "class MLP(Module):\n",
    "\n",
    "    def __init__(self, nin, nouts):\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1], nonlin=(i!=len(nouts)-1)) for i in range(len(nouts))]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for l in self.layers:\n",
    "            params += l.parameters()\n",
    "        return params\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr = '\\n'.join(str(layer) for layer in self.layers)\n",
    "        return f\"MLP of [{repr}]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkkaE1V1m5i5"
   },
   "source": [
    "## Обучение многослойного перцептрона"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWy-H8eCn2zm"
   },
   "source": [
    "Сам перцептрон"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3La6nRi4m920",
    "outputId": "aa871fdc-ce8b-4775-8214-914e96d5da95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP of [Layer of [ReLUNeuron(3), ReLUNeuron(3), ReLUNeuron(3), ReLUNeuron(3)]\n",
      "Layer of [ReLUNeuron(4), ReLUNeuron(4), ReLUNeuron(4), ReLUNeuron(4)]\n",
      "Layer of [LinearNeuron(4)]]\n",
      "number of parameters 41\n"
     ]
    }
   ],
   "source": [
    "model = MLP(3, [4, 4, 1])\n",
    "print(model)\n",
    "print(\"number of parameters\", len(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvkZVOLcnvqu"
   },
   "source": [
    "Набор данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLJULsNanpVC"
   },
   "outputs": [],
   "source": [
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OuCTaTB8n5l0",
    "outputId": "596960ad-8414-4946-f0a8-e14c09e55160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 1889.0, accuracy 0.0%\n",
      "step 400 loss 0.8491560611586679, accuracy 0.0%\n",
      "step 800 loss 0.5598632663471556, accuracy 25.0%\n",
      "step 1200 loss 0.13218400504446332, accuracy 75.0%\n",
      "step 1600 loss 0.05611870318542738, accuracy 100.0%\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "for k in range(2_000):\n",
    "\n",
    "    # forward\n",
    "    out = [model(x) for x in xs]\n",
    "\n",
    "    # calculate loss (mean square error)\n",
    "    acc = 0\n",
    "    total_loss = 0\n",
    "    for y_pred, y_true in zip(out, ys):\n",
    "      total_loss += (y_pred - y_true) ** 2\n",
    "      acc += round(y_pred.data) == y_true\n",
    "    total_loss = total_loss / len(ys)\n",
    "    acc = acc / len(ys)\n",
    "    history.append(total_loss.data)\n",
    "    \n",
    "    # backward (zero_grad + backward)\n",
    "    model.zero_grad()\n",
    "    total_loss.backward()\n",
    "    \n",
    "    # update\n",
    "    learning_rate = 0.001\n",
    "    for p in model.parameters():\n",
    "        p.data = p.data - learning_rate * p.grad\n",
    "    \n",
    "    \n",
    "    if k % 400 == 0:\n",
    "        print(f\"step {k} loss {total_loss.data}, accuracy {acc*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "bdUspbivGbDS",
    "outputId": "d3ff9ab1-5c3b-422f-fa78-556d8bf35116"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"log(loss)\")\n",
    "plt.plot(range(len(history)), [np.log(h) for h in history])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4maaWL5yg-f"
   },
   "source": [
    "# Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yyK39RYo084"
   },
   "source": [
    "**Домашнее задание 1.** Доделать практику. Оформить код в три отдельных модуля `autograd`, `nn`, `train`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdzPyQ-hylKH"
   },
   "source": [
    "**Домашнее задание 2 (Опционально).** Создать свою функцию softmax, наследуемую от `torch.autograd.Function` и имплементировать forward и backward проход. Сравнить со стандартной функцией в Pytorch. \n",
    "[Создание функций](https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html) [Софтмакс](https://congyuzhou.medium.com/softmax-3408fb42d55a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGMpj9Pf61n2"
   },
   "outputs": [],
   "source": [
    "# Ваш код"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VPpRO6H6SHF"
   },
   "source": [
    "**Домашнее задание 3 (Опционально).** Добавить функцию софтмакс в собственну библиотеку автоматического дифференцирования. Сравнить с пунктом 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2YJfxtqSphFs"
   },
   "outputs": [],
   "source": [
    "# Ваш код"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nRRgw0HNsr_a"
   },
   "source": [
    "**Домашнее задание 4 (Опционально).** Добавить визуализацию обучения. Потом мы пройдем более подробно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5AWW52REfn5"
   },
   "source": [
    "https://docs.wandb.ai/guides/integrations/pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekFfy3cWVOIW"
   },
   "source": [
    "https://docs.wandb.ai/ref/python/watch  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9G4SOp28ok0o"
   },
   "source": [
    "https://docs.wandb.ai/guides/track/jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lumiR8oykL04"
   },
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xw3c6P7BkP9b"
   },
   "outputs": [],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udPv0ufwkxOv"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "run = wandb.init(project=\"polynom_learning_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xtpc9MAUodNs"
   },
   "outputs": [],
   "source": [
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MRuSrP7JQ00i",
    "1b95Z8u7Q3OL"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
